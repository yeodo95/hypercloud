apiVersion: tmax.io/v1
kind: ClusterTemplate
metadata:
  name: prometheus-template
  resourceVersion: '220671718'
  selfLink: /apis/tmax.io/v1/clustertemplates/prometheus-template
  uid: 71ad92b8-f6ab-4e64-b60d-e9dfa43ff9d7
imageUrl: 'https://folo.co.kr/img/gm_noimage.png'
longDescription: prometheus-template
markdownDescription: prometheus-template
objectKinds:
  - ServiceAccount
  - Role
  - RoleBinding
  - Role
  - RoleBinding
  - Prometheus
  - Service
  - Ingress
  - ServiceMonitor
  - PrometheusRule
  - PersistentVolumeClaim
  - Service
  - Ingress
  - ServiceAccount
  - ServiceMonitor
  - Deployment
objects:
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: '${APP_NAME}-account'
      namespace: '${NAMESPACE}'
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: '${APP_NAME}-role'
      namespace: '${NAMESPACE}'
    rules:
      - apiGroups:
          - ''
        resources:
          - services
          - endpoints
          - pods
          - servicemonitor
          - podmonitor
        verbs:
          - get
          - list
          - watch
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: '${APP_NAME}-binding'
    namespace: '${NAMESPACE}'
    roleRef:
      kind: Role
      apiGroup: rbac.authorization.k8s.io
      name: '${APP_NAME}-role'
    subjects:
    - kind: ServiceAccount
      name: '${APP_NAME}-account'
      namespace: '${NAMESPACE}'
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: '${APP_NAME}-role-config'
      namespace: '${NAMESPACE}'
    rules:
      - apiGroups:
          - ''
        resources:
          - configmaps
        verbs:
          - get
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: '${APP_NAME}-config-binding'
      namespace: '${NAMESPACE}'
    roleRef:
      kind: Role
      apiGroup: rbac.authorization.k8s.io
      name: '${APP_NAME}-role-config'
    subjects:
      - kind: ServiceAccount
        name: '${APP_NAME}-account'
        namespace: '${NAMESPACE}'
  - apiVersion: monitoring.coreos.com/v1
    kind: Prometheus
    metadata:
      labels:
        prometheus: k8s
      name: k8s
      namespace: '${NAMESPACE}'
    spec:
      alerting:
        alertmanagers:
          - name: alertmanager-main
            namespace: '${NAMESPACE}'
            port: web
      image: 'quay.io/prometheus/prometheus:v2.11.0'
      nodeSelector:
        kubernetes.io/os: linux
      podMonitorSelector: {}
      replicas: 1
      resources:
        requests:
          cpu: 10m
          memory: 2Gi
      ruleSelector:
        matchLabels:
          prometheus: k8s
          role: alert-rules
      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
      serviceAccountName: '${APP_NAME}-account'
      serviceMonitorNamespaceSelector: {}
      serviceMonitorSelector:
        matchNames:
          - '${NAMESPACE}'
      storage:
        volumeClaimTemplate:
          spec:
            accessModes:
              - ReadWriteMany
            resources:
              requests:
                storage: 10Gi
      version: v2.11.0
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        prometheus: k8s
      name: '${APP_NAME}-service'
      namespace: '${NAMESPACE}'
    spec:
      ports:
        - name: web
          port: 9090
          targetPort: web
      selector:
        app: prometheus
        prometheus: k8s
      sessionAffinity: ClientIP
      type: NodePort
  - apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: '${APP_NAME}'
      namespace: '${NAMESPACE}'
    spec:
      ingressClassName: tmax
      rules:
        - host: prometheus.${GATEWAY_ADDRESS}
          http:
            paths:
              - backend:
                  serviceName: '${APP_NAME}-service'
                  servicePort: 9090
                path: /
                pathType: Prefix
    status:
      loadBalancer:
        ingress:
          - ip: '${INGRESS_IP}'
  - apiVersion: monitoring.coreos.com/v1
    kind: ServiceMonitor
    metadata:
      labels:
        k8s-app: prometheus
      name: prometheus
      namespace: '${NAMESPACE}'
    spec:
      endpoints:
        - interval: 30s
          port: web
      selector:
        matchLabels:
          prometheus: k8s
  - apiVersion: monitoring.coreos.com/v1
    kind: PrometheusRule
    metadata:
      labels:
        prometheus: k8s
        role: alert-rules
      name: prometheus-k8s-rules
      namespace: '${NAMESPACE}'
    spec:
      groups:
        - name: kubernetes.rules
          rules:
            - expr: >-
                sum(container_memory_usage_bytes{container_name!="POD",pod!=""})
                BY (pod, namespace)
              record: 'pod:container_memory_usage_bytes:sum'
            - expr: >-
                sum(container_spec_cpu_shares{container_name!="POD",pod!=""}) BY
                (pod, namespace)
              record: 'pod:container_spec_cpu_shares:sum'
            - expr: >-
                sum(rate(container_cpu_usage_seconds_total{container!="POD",pod!="",container!=""}[5m]))
                BY (pod, namespace)
              record: 'pod:container_cpu_usage:sum'
            - expr: >-
                sum(container_fs_usage_bytes{container_name!="POD",pod!=""}) BY
                (pod, namespace)
              record: 'pod:container_fs_usage_bytes:sum'
            - expr: 'sum(container_memory_usage_bytes{pod!=""}) BY (namespace)'
              record: 'namespace:container_memory_usage_bytes:sum'
            - expr: 'sum(container_spec_cpu_shares{pod!=""}) BY (namespace)'
              record: 'namespace:container_spec_cpu_shares:sum'
            - expr: >-
                sum(rate(container_cpu_usage_seconds_total{pod!=""}[5m])) BY
                (namespace)
              record: 'namespace:container_cpu_usage:sum'
            - expr: >-
                sum(container_memory_usage_bytes{container_name!="POD",pod!=""})
                BY (cluster) / sum(machine_memory_bytes) BY (cluster)
              record: 'cluster:memory_usage:ratio'
            - expr: >-
                sum(container_spec_cpu_shares{container_name!="POD",pod!=""}) /
                1000 / sum(machine_cpu_cores)
              record: 'cluster:container_spec_cpu_shares:ratio'
            - expr: >-
                sum(rate(container_cpu_usage_seconds_total{container_name!="POD",pod!=""}[5m]))
                / sum(machine_cpu_cores)
              record: 'cluster:container_cpu_usage:ratio'
            - alert: ClusterMonitoringOperatorErrors
              annotations:
                message: >-
                  Cluster Monitoring Operator is experiencing {{ printf "%0.0f"
                  $value }}% errors.
              expr: >-
                sum(rate(cluster_monitoring_operator_reconcile_errors_total[15m]))
                * 100 /
                sum(rate(cluster_monitoring_operator_reconcile_attempts_total[15m]))
                > 10
              for: 15m
              labels:
                severity: critical
        - name: node-exporter.rules
          rules:
            - expr: |
                count (
                  node_cpu_seconds_total{job="node-exporter",mode="idle"}
                )
              record: 'instance:node_cpu:num'
            - expr: |
                sum(
                  rate(
                    node_cpu{job="node-exporter",mode!="idle"}[5m])
                    ) 
              record: 'instance_cpu:node_cpu_seconds_not_idle:rate5m'
            - expr: |
                1 - avg(
                  rate(node_cpu{job="node-exporter", mode="idle"}[5m])
                )
              record: 'instance:node_cpu_utilisation:rate5m'
            - expr: |
                sum(
                  rate(node_cpu{job="node-exporter"}[5m])
                  )
              record: 'instance_mode:node_cpu_seconds:rate5m'
            - expr: |
                sum(
                  instance_mode:node_cpu:rate5m{job="node-exporter",mode!="idle"}
                  ) without (mode) / instance:node_num_cpu:sum
              record: 'instance:node_cpu_utilization:ratio'
            - expr: |
                (
                  node_load1{job="node-exporter"}
                /
                  instance:node_num_cpu:sum{job="node-exporter"}
                )
              record: 'instance:node_load1_per_cpu:ratio'
            - expr: |
                1 - (
                  node_memory_MemAvailable_bytes{job="node-exporter"}
                /
                  node_memory_MemTotal_bytes{job="node-exporter"}
                )
              record: 'instance:node_memory_utilisation:ratio'
            - expr: |
                rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
              record: 'instance:node_vmstat_pgmajfault:rate1m'
            - expr: >
                rate(node_disk_io_time_seconds_total{job="node-exporter",
                device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
              record: 'instance_device:node_disk_io_time_seconds:rate1m'
            - expr: >
                rate(node_disk_io_time_weighted_seconds_total{job="node-exporter",
                device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
              record: 'instance_device:node_disk_io_time_weighted_seconds:rate1m'
            - expr: |
                sum without (device) (
                  rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
                )
              record: 'instance:node_network_receive_bytes_excluding_lo:rate1m'
            - expr: |
                sum without (device) (
                  rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
                )
              record: 'instance:node_network_transmit_bytes_excluding_lo:rate1m'
            - expr: |
                sum without (device) (
                  rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
                )
              record: 'instance:node_network_receive_drop_excluding_lo:rate1m'
            - expr: |
                sum without (device) (
                  rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
                )
              record: 'instance:node_network_transmit_drop_excluding_lo:rate1m'
        - name: node_exporter-16-bcache
          rules:
            - expr: node_bcache_cache_read_races
              record: node_bcache_cache_read_races_total
        - name: node_exporter-16-buddyinfo
          rules:
            - expr: node_buddyinfo_blocks
              record: node_buddyinfo_count
        - name: node_exporter-16-stat
          rules:
            - expr: node_boot_time_seconds
              record: node_boot_time
            - expr: node_context_switches_total
              record: node_context_switches
            - expr: node_forks_total
              record: node_forks
            - expr: node_intr_total
              record: node_intr
        - name: node_exporter-16-cpu
          rules:
            - expr: >-
                label_replace(node_cpu_seconds_total, "cpu", "$1", "cpu",
                "cpu(.+)")
              record: node_cpu
        - name: node_exporter-16-diskstats
          rules:
            - expr: node_disk_read_bytes_total
              record: node_disk_bytes_read
            - expr: node_disk_written_bytes_total
              record: node_disk_bytes_written
            - expr: node_disk_io_time_seconds_total * 1000
              record: node_disk_io_time_ms
            - expr: node_disk_io_time_weighted_seconds_total
              record: node_disk_io_time_weighted
            - expr: node_disk_reads_completed_total
              record: node_disk_reads_completed
            - expr: node_disk_reads_merged_total
              record: node_disk_reads_merged
            - expr: node_disk_read_time_seconds_total * 1000
              record: node_disk_read_time_ms
            - expr: node_disk_writes_completed_total
              record: node_disk_writes_completed
            - expr: node_disk_writes_merged_total
              record: node_disk_writes_merged
            - expr: node_disk_write_time_seconds_total * 1000
              record: node_disk_write_time_ms
        - name: node_exporter-16-filesystem
          rules:
            - expr: node_filesystem_free_bytes
              record: node_filesystem_free
            - expr: node_filesystem_avail_bytes
              record: node_filesystem_avail
            - expr: node_filesystem_size_bytes
              record: node_filesystem_size
        - name: node_exporter-16-infiniband
          rules:
            - expr: node_infiniband_port_data_received_bytes_total
              record: node_infiniband_port_data_received_bytes
            - expr: node_infiniband_port_data_transmitted_bytes_total
              record: node_infiniband_port_data_transmitted_bytes
        - name: node_exporter-16-interrupts
          rules:
            - expr: node_interrupts_total
              record: node_interrupts
        - name: node_exporter-16-memory
          rules:
            - expr: node_memory_Active_bytes
              record: node_memory_Active
            - expr: node_memory_Active_anon_bytes
              record: node_memory_Active_anon
            - expr: node_memory_Active_file_bytes
              record: node_memory_Active_file
            - expr: node_memory_AnonHugePages_bytes
              record: node_memory_AnonHugePages
            - expr: node_memory_AnonPages_bytes
              record: node_memory_AnonPages
            - expr: node_memory_Bounce_bytes
              record: node_memory_Bounce
            - expr: node_memory_Buffers_bytes
              record: node_memory_Buffers
            - expr: node_memory_Cached_bytes
              record: node_memory_Cached
            - expr: node_memory_CommitLimit_bytes
              record: node_memory_CommitLimit
            - expr: node_memory_Committed_AS_bytes
              record: node_memory_Committed_AS
            - expr: node_memory_DirectMap2M_bytes
              record: node_memory_DirectMap2M
            - expr: node_memory_DirectMap4k_bytes
              record: node_memory_DirectMap4k
            - expr: node_memory_Dirty_bytes
              record: node_memory_Dirty
            - expr: node_memory_HardwareCorrupted_bytes
              record: node_memory_HardwareCorrupted
            - expr: node_memory_Hugepagesize_bytes
              record: node_memory_Hugepagesize
            - expr: node_memory_Inactive_bytes
              record: node_memory_Inactive
            - expr: node_memory_Inactive_anon_bytes
              record: node_memory_Inactive_anon
            - expr: node_memory_Inactive_file_bytes
              record: node_memory_Inactive_file
            - expr: node_memory_KernelStack_bytes
              record: node_memory_KernelStack
            - expr: node_memory_Mapped_bytes
              record: node_memory_Mapped
            - expr: node_memory_MemAvailable_bytes
              record: node_memory_MemAvailable
            - expr: node_memory_MemFree_bytes
              record: node_memory_MemFree
            - expr: node_memory_MemTotal_bytes
              record: node_memory_MemTotal
            - expr: node_memory_Mlocked_bytes
              record: node_memory_Mlocked
            - expr: node_memory_NFS_Unstable_bytes
              record: node_memory_NFS_Unstable
            - expr: node_memory_PageTables_bytes
              record: node_memory_PageTables
            - expr: node_memory_Shmem_bytes
              record: node_memory_Shmem
            - expr: node_memory_Slab_bytes
              record: node_memory_Slab
            - expr: node_memory_SReclaimable_bytes
              record: node_memory_SReclaimable
            - expr: node_memory_SUnreclaim_bytes
              record: node_memory_SUnreclaim
            - expr: node_memory_SwapCached_bytes
              record: node_memory_SwapCached
            - expr: node_memory_SwapFree_bytes
              record: node_memory_SwapFree
            - expr: node_memory_SwapTotal_bytes
              record: node_memory_SwapTotal
            - expr: node_memory_Unevictable_bytes
              record: node_memory_Unevictable
            - expr: node_memory_VmallocChunk_bytes
              record: node_memory_VmallocChunk
            - expr: node_memory_VmallocTotal_bytes
              record: node_memory_VmallocTotal
            - expr: node_memory_VmallocUsed_bytes
              record: node_memory_VmallocUsed
            - expr: node_memory_Writeback_bytes
              record: node_memory_Writeback
            - expr: node_memory_WritebackTmp_bytes
              record: node_memory_WritebackTmp
        - name: node_exporter-16-network
          rules:
            - expr: node_network_receive_bytes_total
              record: node_network_receive_bytes
            - expr: node_network_receive_compressed_total
              record: node_network_receive_compressed
            - expr: node_network_receive_drop_total
              record: node_network_receive_drop
            - expr: node_network_receive_errs_total
              record: node_network_receive_errs
            - expr: node_network_receive_fifo_total
              record: node_network_receive_fifo
            - expr: node_network_receive_frame_total
              record: node_network_receive_frame
            - expr: node_network_receive_multicast_total
              record: node_network_receive_multicast
            - expr: node_network_receive_packets_total
              record: node_network_receive_packets
            - expr: node_network_transmit_bytes_total
              record: node_network_transmit_bytes
            - expr: node_network_transmit_compressed_total
              record: node_network_transmit_compressed
            - expr: node_network_transmit_drop_total
              record: node_network_transmit_drop
            - expr: node_network_transmit_errs_total
              record: node_network_transmit_errs
            - expr: node_network_transmit_fifo_total
              record: node_network_transmit_fifo
            - expr: node_network_transmit_frame_total
              record: node_network_transmit_frame
            - expr: node_network_transmit_multicast_total
              record: node_network_transmit_multicast
            - expr: node_network_transmit_packets_total
              record: node_network_transmit_packets
        - name: node_exporter-16-nfs
          rules:
            - expr: node_nfs_connections_total
              record: node_nfs_net_connections
            - expr: node_nfs_packets_total
              record: node_nfs_net_reads
            - expr: >-
                label_replace(label_replace(node_nfs_requests_total, "proto",
                "$1", "version", "(.+)"), "method", "$1", "procedure", "(.+)")
              record: node_nfs_procedures
            - expr: node_nfs_rpc_authentication_refreshes_total
              record: node_nfs_rpc_authentication_refreshes
            - expr: node_nfs_rpcs_total
              record: node_nfs_rpc_operations
            - expr: node_nfs_rpc_retransmissions_total
              record: node_nfs_rpc_retransmissions
        - name: node_exporter-16-textfile
          rules:
            - expr: node_textfile_mtime_seconds
              record: node_textfile_mtime
        - name: kube-apiserver.rules
          rules:
            - expr: >
                histogram_quantile(0.99,
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: >-
                cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.9,
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: >-
                cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.5,
                sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: >-
                cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        - name: k8s.rules
          rules:
            - expr: >
                sum(rate(container_cpu_usage_seconds_total{job="kubelet",
                image!="", container!="POD"}[5m])) by (namespace)
              record: 'namespace:container_cpu_usage_seconds_total:sum_rate'
            - expr: >
                sum by (namespace, pod, container) (
                  rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])
                ) * on (namespace, pod) group_left(node) max by(namespace, pod,
                node) (kube_pod_info)
              record: >-
                node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
            - expr: >
                container_memory_working_set_bytes{job="kubelet", image!=""}

                * on (namespace, pod) group_left(node) max by(namespace, pod,
                node) (kube_pod_info)
              record: 'node_namespace_pod_container:container_memory_working_set_bytes'
            - expr: >
                container_memory_rss{job="kubelet", image!=""}

                * on (namespace, pod) group_left(node) max by(namespace, pod,
                node) (kube_pod_info)
              record: 'node_namespace_pod_container:container_memory_rss'
            - expr: >
                container_memory_cache{job="kubelet", image!=""}

                * on (namespace, pod) group_left(node) max by(namespace, pod,
                node) (kube_pod_info)
              record: 'node_namespace_pod_container:container_memory_cache'
            - expr: >
                container_memory_swap{job="kubelet", image!=""}

                * on (namespace, pod) group_left(node) max by(namespace, pod,
                node) (kube_pod_info)
              record: 'node_namespace_pod_container:container_memory_swap'
            - expr: >
                sum(container_memory_usage_bytes{job="kubelet", image!="",
                container!="POD"}) by (namespace)
              record: 'namespace:container_memory_usage_bytes:sum'
            - expr: |
                sum by (namespace, label_name) (
                    sum(kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)) by (namespace, pod)
                  * on (namespace, pod)
                    group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
                )
              record: 'namespace:kube_pod_container_resource_requests_memory_bytes:sum'
            - expr: |
                sum by (namespace, label_name) (
                    sum(kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)) by (namespace, pod)
                  * on (namespace, pod)
                    group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
                )
              record: 'namespace:kube_pod_container_resource_requests_cpu_cores:sum'
            - expr: |
                sum(
                  label_replace(
                    label_replace(
                      kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                      "replicaset", "$1", "owner_name", "(.*)"
                    ) * on(replicaset, namespace) group_left(owner_name) kube_replicaset_owner{job="kube-state-metrics"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                ) by (namespace, workload, pod)
              labels:
                workload_type: deployment
              record: mixin_pod_workload
            - expr: |
                sum(
                  label_replace(
                    kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                ) by (namespace, workload, pod)
              labels:
                workload_type: daemonset
              record: mixin_pod_workload
            - expr: |
                sum(
                  label_replace(
                    kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
                    "workload", "$1", "owner_name", "(.*)"
                  )
                ) by (namespace, workload, pod)
              labels:
                workload_type: statefulset
              record: mixin_pod_workload
        - name: kube-scheduler.rules
          rules:
            - expr: >
                histogram_quantile(0.99,
                sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: >-
                cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.99,
                sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: >-
                cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.99,
                sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.99'
              record: >-
                cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.9,
                sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: >-
                cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.9,
                sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: >-
                cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.9,
                sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.9'
              record: >-
                cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.5,
                sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: >-
                cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.5,
                sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: >-
                cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
            - expr: >
                histogram_quantile(0.5,
                sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m]))
                without(instance, pod))
              labels:
                quantile: '0.5'
              record: >-
                cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - name: node.rules
          rules:
            - expr: sum(min(kube_pod_info) by (node))
              record: ':kube_pod_info_node_count:'
            - expr: >
                max(label_replace(kube_pod_info{job="kube-state-metrics"},
                "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
              record: 'node_namespace_pod:kube_pod_info:'
            - expr: |
                count by (node) (sum by (node, cpu) (
                  node_cpu{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                ))
              record: 'node:node_num_cpu:sum'
            - expr: |
                1 - avg(rate(node_cpu{job="node-exporter",mode="idle"}[1m]))
              record: ':node_cpu_utilisation:avg1m'
            - expr: |
                1 - avg by (node) (
                  rate(node_cpu{job="node-exporter",mode="idle"}[1m])
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:)
              record: 'node:node_cpu_utilisation:avg1m'
            - expr: |
                sum(node_load1{job="node-exporter"})
                /
                sum(node:node_num_cpu:sum)
              record: ':node_cpu_saturation_load1:'
            - expr: |
                sum by (node) (
                  node_load1{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
                /
                node:node_num_cpu:sum
              record: 'node:node_cpu_saturation_load1:'
            - expr: >
                1 -

                sum(node_memory_MemFree{job="node-exporter"} +
                node_memory_Cached{job="node-exporter"} +
                node_memory_Buffers{job="node-exporter"})

                /

                sum(node_memory_MemTotal{job="node-exporter"})
              record: ':node_memory_utilisation:'
            - expr: >
                sum(node_memory_MemFree{job="node-exporter"} +
                node_memory_Cached{job="node-exporter"} +
                node_memory_Buffers{job="node-exporter"})
              record: ':node_memory_MemFreeCachedBuffers:sum'
            - expr: |
                sum(node_memory_MemTotal{job="node-exporter"})
              record: ':node_memory_MemTotal:sum'
            - expr: |
                sum by (node) (
                  (node_memory_MemFree{job="node-exporter"} + node_memory_Cached{job="node-exporter"} + node_memory_Buffers{job="node-exporter"})
                  * on (namespace, pod) group_left(node)
                    node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_memory_bytes_available:sum'
            - expr: |
                sum by (node) (
                  node_memory_MemTotal{job="node-exporter"}
                  * on (namespace, pod) group_left(node)
                    node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_memory_bytes_total:sum'
            - expr: >
                (node:node_memory_bytes_total:sum -
                node:node_memory_bytes_available:sum)

                /

                scalar(sum(node:node_memory_bytes_total:sum))
              record: 'node:node_memory_utilisation:ratio'
            - expr: |
                1e3 * sum(
                  (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
                 + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
                )
              record: ':node_memory_swap_io_bytes:sum_rate'
            - expr: |
                1 -
                sum by (node) (
                  (node_memory_MemFree{job="node-exporter"} + node_memory_Cached{job="node-exporter"} + node_memory_Buffers{job="node-exporter"})
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
                /
                sum by (node) (
                  node_memory_MemTotal{job="node-exporter"}
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_memory_utilisation:'
            - expr: >
                1 - (node:node_memory_bytes_available:sum /
                node:node_memory_bytes_total:sum)
              record: 'node:node_memory_utilisation_2:'
            - expr: |
                1e3 * sum by (node) (
                  (rate(node_vmstat_pgpgin{job="node-exporter"}[1m])
                 + rate(node_vmstat_pgpgout{job="node-exporter"}[1m]))
                 * on (namespace, pod) group_left(node)
                   node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_memory_swap_io_bytes:sum_rate'
            - expr: >
                avg(irate(node_disk_io_time_ms{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m])
                / 1e3)
              record: ':node_disk_utilisation:avg_irate'
            - expr: |
                avg by (node) (
                  irate(node_disk_io_time_ms{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_disk_utilisation:avg_irate'
            - expr: >
                avg(irate(node_disk_io_time_weighted{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m])
                / 1e3)
              record: ':node_disk_saturation:avg_irate'
            - expr: |
                avg by (node) (
                  irate(node_disk_io_time_weighted{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]) / 1e3
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_disk_saturation:avg_irate'
            - expr: >
                max by (namespace, pod, device)
                ((node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs"}

                - node_filesystem_avail{fstype=~"ext[234]|btrfs|xfs|zfs"})

                / node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs"})
              record: 'node:node_filesystem_usage:'
            - expr: >
                max by (namespace, pod, device)
                (node_filesystem_avail{fstype=~"ext[234]|btrfs|xfs|zfs"} /
                node_filesystem_size{fstype=~"ext[234]|btrfs|xfs|zfs"})
              record: 'node:node_filesystem_avail:'
            - expr: >
                sum(irate(node_network_receive_bytes{job="node-exporter",device="eth0"}[1m]))
                +

                sum(irate(node_network_transmit_bytes{job="node-exporter",device="eth0"}[1m]))
              record: ':node_net_utilisation:sum_irate'
            - expr: |
                sum by (node) (
                  (irate(node_network_receive_bytes{job="node-exporter",device="eth0"}[1m]) +
                  irate(node_network_transmit_bytes{job="node-exporter",device="eth0"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_net_utilisation:sum_irate'
            - expr: >
                sum(irate(node_network_receive_drop{job="node-exporter",device="eth0"}[1m]))
                +

                sum(irate(node_network_transmit_drop{job="node-exporter",device="eth0"}[1m]))
              record: ':node_net_saturation:sum_irate'
            - expr: |
                sum by (node) (
                  (irate(node_network_receive_drop{job="node-exporter",device="eth0"}[1m]) +
                  irate(node_network_transmit_drop{job="node-exporter",device="eth0"}[1m]))
                * on (namespace, pod) group_left(node)
                  node_namespace_pod:kube_pod_info:
                )
              record: 'node:node_net_saturation:sum_irate'
        - name: kube-prometheus-node-recording.rules
          rules:
            - expr: >-
                sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m]))
                BY (instance)
              record: 'instance:node_cpu:rate:sum'
            - expr: >-
                sum((node_filesystem_size_bytes{mountpoint="/"} -
                node_filesystem_free_bytes{mountpoint="/"})) BY (instance)
              record: 'instance:node_filesystem_usage:sum'
            - expr: 'sum(rate(node_network_receive_bytes_total[3m])) BY (instance)'
              record: 'instance:node_network_receive_bytes:rate:sum'
            - expr: 'sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)'
              record: 'instance:node_network_transmit_bytes:rate:sum'
            - expr: >-
                sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
                WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT()
                count(sum(node_cpu_seconds_total) BY (instance, cpu)) BY
                (instance)
              record: 'instance:node_cpu:ratio'
            - expr: >-
                sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
              record: 'cluster:node_cpu:sum_rate5m'
            - expr: >-
                cluster:node_cpu_seconds_total:rate5m /
                count(sum(node_cpu_seconds_total) BY (instance, cpu))
              record: 'cluster:node_cpu:ratio'
        - name: node-exporter
          rules:
            - alert: NodeFilesystemSpaceFillingUp
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available space left and
                  is filling up.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
                summary: >-
                  Filesystem is predicted to run out of space within the next 24
                  hours.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 40
                and
                  predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: warning
            - alert: NodeFilesystemSpaceFillingUp
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available space left and
                  is filling up fast.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemspacefillingup
                summary: >-
                  Filesystem is predicted to run out of space within the next 4
                  hours.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 20
                and
                  predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: critical
            - alert: NodeFilesystemAlmostOutOfSpace
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available space left.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
                summary: Filesystem has less than 5% space left.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: warning
            - alert: NodeFilesystemAlmostOutOfSpace
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available space left.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutofspace
                summary: Filesystem has less than 3% space left.
              expr: |
                (
                  node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: critical
            - alert: NodeFilesystemFilesFillingUp
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available inodes left and
                  is filling up.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
                summary: >-
                  Filesystem is predicted to run out of inodes within the next
                  24 hours.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
                and
                  predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: warning
            - alert: NodeFilesystemFilesFillingUp
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available inodes left and
                  is filling up fast.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemfilesfillingup
                summary: >-
                  Filesystem is predicted to run out of inodes within the next 4
                  hours.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
                and
                  predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: critical
            - alert: NodeFilesystemAlmostOutOfFiles
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available inodes left.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
                summary: Filesystem has less than 5% inodes left.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: warning
            - alert: NodeFilesystemAlmostOutOfFiles
              annotations:
                description: >-
                  Filesystem on {{ $labels.device }} at {{ $labels.instance }}
                  has only {{ printf "%.2f" $value }}% available inodes left.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodefilesystemalmostoutoffiles
                summary: Filesystem has less than 3% inodes left.
              expr: |
                (
                  node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
                and
                  node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
                )
              for: 1h
              labels:
                severity: critical
            - alert: NodeNetworkReceiveErrs
              annotations:
                description: >-
                  {{ $labels.instance }} interface {{ $labels.device }} has
                  encountered {{ printf "%.0f" $value }} receive errors in the
                  last two minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworkreceiveerrs
                summary: Network interface is reporting many receive errors.
              expr: |
                increase(node_network_receive_errs_total[2m]) > 10
              for: 1h
              labels:
                severity: warning
            - alert: NodeNetworkTransmitErrs
              annotations:
                description: >-
                  {{ $labels.instance }} interface {{ $labels.device }} has
                  encountered {{ printf "%.0f" $value }} transmit errors in the
                  last two minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodenetworktransmiterrs
                summary: Network interface is reporting many transmit errors.
              expr: |
                increase(node_network_transmit_errs_total[2m]) > 10
              for: 1h
              labels:
                severity: warning
        - name: kubernetes-apps
          rules:
            - alert: KubePodCrashLooping
              annotations:
                message: >-
                  Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{
                  $labels.container }}) is restarting {{ printf "%.2f" $value }}
                  times / 5 minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
              expr: >
                rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m])
                * 60 * 5 > 0
              for: 15m
              labels:
                app: test
                severity: critical
            - alert: KubePodNotReady
              annotations:
                message: >-
                  Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a
                  non-ready state for longer than 15 minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
              expr: >
                sum by (namespace, pod)
                (kube_pod_status_phase{job="kube-state-metrics",
                phase=~"Failed|Pending|Unknown"} * on(namespace, pod)
                group_left(owner_kind) kube_pod_owner{owner_kind!="Job"}) > 0
              for: 15m
              labels:
                severity: critical
            - alert: KubeDeploymentGenerationMismatch
              annotations:
                message: >-
                  Deployment generation for {{ $labels.namespace }}/{{
                  $labels.deployment }} does not match, this indicates that the
                  Deployment has failed but has not been rolled back.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
              expr: >
                kube_deployment_status_observed_generation{job="kube-state-metrics"}
                  !=
                kube_deployment_metadata_generation{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: critical
            - alert: KubeDeploymentReplicasMismatch
              annotations:
                message: >-
                  Deployment {{ $labels.namespace }}/{{ $labels.deployment }}
                  has not matched the expected number of replicas for longer
                  than 15 minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
              expr: >
                kube_deployment_spec_replicas{job="kube-state-metrics"}
                  !=
                kube_deployment_status_replicas_available{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: critical
            - alert: KubeStatefulSetReplicasMismatch
              annotations:
                message: >-
                  StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
                  has not matched the expected number of replicas for longer
                  than 15 minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
              expr: |
                kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
                  !=
                kube_statefulset_status_replicas{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: critical
            - alert: KubeStatefulSetGenerationMismatch
              annotations:
                message: >-
                  StatefulSet generation for {{ $labels.namespace }}/{{
                  $labels.statefulset }} does not match, this indicates that the
                  StatefulSet has failed but has not been rolled back.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
              expr: >
                kube_statefulset_status_observed_generation{job="kube-state-metrics"}
                  !=
                kube_statefulset_metadata_generation{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: critical
            - alert: KubeStatefulSetUpdateNotRolledOut
              annotations:
                message: >-
                  StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
                  update has not been rolled out.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
              expr: |
                max without (revision) (
                  kube_statefulset_status_current_revision{job="kube-state-metrics"}
                    unless
                  kube_statefulset_status_update_revision{job="kube-state-metrics"}
                )
                  *
                (
                  kube_statefulset_replicas{job="kube-state-metrics"}
                    !=
                  kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
                )
              for: 15m
              labels:
                severity: critical
            - alert: KubeDaemonSetRolloutStuck
              annotations:
                message: >-
                  Only {{ $value | humanizePercentage }} of the desired Pods of
                  DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are
                  scheduled and ready.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
              expr: >
                kube_daemonset_status_number_ready{job="kube-state-metrics"}
                  /
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
                < 1.00
              for: 15m
              labels:
                severity: critical
            - alert: KubeContainerWaiting
              annotations:
                message: >-
                  Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{
                  $labels.container}} has been in waiting state for longer than
                  1 hour.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontainerwaiting
              expr: >
                sum by (namespace, pod, container)
                (kube_pod_container_status_waiting_reason{job="kube-state-metrics"})
                > 0
              for: 1h
              labels:
                severity: warning
            - alert: KubeDaemonSetNotScheduled
              annotations:
                message: >-
                  {{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{
                  $labels.daemonset }} are not scheduled.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
              expr: >
                kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
                  -
                kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"}
                > 0
              for: 10m
              labels:
                severity: warning
            - alert: KubeDaemonSetMisScheduled
              annotations:
                message: >-
                  {{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{
                  $labels.daemonset }} are running where they are not supposed
                  to run.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
              expr: >
                kube_daemonset_status_number_misscheduled{job="kube-state-metrics"}
                > 0
              for: 10m
              labels:
                severity: warning
            - alert: KubeCronJobRunning
              annotations:
                message: >-
                  CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is
                  taking more than 1h to complete.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
              expr: >
                time() -
                kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
              for: 1h
              labels:
                severity: warning
            - alert: KubeJobCompletion
              annotations:
                message: >-
                  Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking
                  more than one hour to complete.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
              expr: >
                kube_job_spec_completions{job="kube-state-metrics"} -
                kube_job_status_succeeded{job="kube-state-metrics"}  > 0
              for: 1h
              labels:
                severity: warning
            - alert: KubeJobFailed
              annotations:
                message: >-
                  Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to
                  complete.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
              expr: |
                kube_job_failed{job="kube-state-metrics"}  > 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeHpaReplicasMismatch
              annotations:
                message: >-
                  HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched
                  the desired number of replicas for longer than 15 minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpareplicasmismatch
              expr: |
                (kube_hpa_status_desired_replicas{job="kube-state-metrics"}
                  !=
                kube_hpa_status_current_replicas{job="kube-state-metrics"})
                  and
                changes(kube_hpa_status_current_replicas[15m]) == 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeHpaMaxedOut
              annotations:
                message: >-
                  HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running
                  at max replicas for longer than 15 minutes.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubehpamaxedout
              expr: |
                kube_hpa_status_current_replicas{job="kube-state-metrics"}
                  ==
                kube_hpa_spec_max_replicas{job="kube-state-metrics"}
              for: 15m
              labels:
                severity: warning
        - name: kubernetes-resources
          rules:
            - alert: KubeCPUOvercommit
              annotations:
                message: >-
                  Cluster has overcommitted CPU resource requests for Pods and
                  cannot tolerate node failure.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
              expr: >
                sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
                  /
                sum(kube_node_status_allocatable_cpu_cores)
                  >
                (count(kube_node_status_allocatable_cpu_cores)-1) /
                count(kube_node_status_allocatable_cpu_cores)
              for: 5m
              labels:
                severity: warning
            - alert: KubeMemOvercommit
              annotations:
                message: >-
                  Cluster has overcommitted memory resource requests for Pods
                  and cannot tolerate node failure.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
              expr: >
                sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
                  /
                sum(kube_node_status_allocatable_memory_bytes)
                  >
                (count(kube_node_status_allocatable_memory_bytes)-1)
                  /
                count(kube_node_status_allocatable_memory_bytes)
              for: 5m
              labels:
                severity: warning
            - alert: KubeCPUOvercommit
              annotations:
                message: >-
                  Cluster has overcommitted CPU resource requests for
                  Namespaces.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
              expr: >
                sum(kube_resourcequota{job="kube-state-metrics", type="hard",
                resource="cpu"})
                  /
                sum(kube_node_status_allocatable_cpu_cores)
                  > 1.5
              for: 5m
              labels:
                severity: warning
            - alert: KubeMemOvercommit
              annotations:
                message: >-
                  Cluster has overcommitted memory resource requests for
                  Namespaces.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
              expr: >
                sum(kube_resourcequota{job="kube-state-metrics", type="hard",
                resource="memory"})
                  /
                sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
                  > 1.5
              for: 5m
              labels:
                severity: warning
            - alert: KubeQuotaExceeded
              annotations:
                message: >-
                  Namespace {{ $labels.namespace }} is using {{ $value |
                  humanizePercentage }} of its {{ $labels.resource }} quota.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
              expr: |
                kube_resourcequota{job="kube-state-metrics", type="used"}
                  / ignoring(instance, job, type)
                (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                  > 0.90
              for: 15m
              labels:
                severity: warning
            - alert: CPUThrottlingHigh
              annotations:
                message: >-
                  {{ $value | humanizePercentage }} throttling of CPU in
                  namespace {{ $labels.namespace }} for container {{
                  $labels.container }} in pod {{ $labels.pod }}.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
              expr: >
                sum(increase(container_cpu_cfs_throttled_periods_total{container!="",
                }[5m])) by (container, pod, namespace)
                  /
                sum(increase(container_cpu_cfs_periods_total{}[5m])) by
                (container, pod, namespace)
                  > ( 25 / 100 )
              for: 15m
              labels:
                severity: warning
        - name: kubernetes-storage
          rules:
            - alert: KubePersistentVolumeUsageCritical
              annotations:
                message: >-
                  The PersistentVolume claimed by {{
                  $labels.persistentvolumeclaim }} in Namespace {{
                  $labels.namespace }} is only {{ $value | humanizePercentage }}
                  free.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
              expr: |
                kubelet_volume_stats_available_bytes{job="kubelet"}
                  /
                kubelet_volume_stats_capacity_bytes{job="kubelet"}
                  < 0.03
              for: 1m
              labels:
                severity: critical
            - alert: KubePersistentVolumeFullInFourDays
              annotations:
                message: >-
                  Based on recent sampling, the PersistentVolume claimed by {{
                  $labels.persistentvolumeclaim }} in Namespace {{
                  $labels.namespace }} is expected to fill up within four days.
                  Currently {{ $value | humanizePercentage }} is available.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
              expr: >
                (
                  kubelet_volume_stats_available_bytes{job="kubelet"}
                    /
                  kubelet_volume_stats_capacity_bytes{job="kubelet"}
                ) < 0.15

                and

                predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h],
                4 * 24 * 3600) < 0
              for: 1h
              labels:
                severity: critical
            - alert: KubePersistentVolumeErrors
              annotations:
                message: >-
                  The persistent volume {{ $labels.persistentvolume }} has
                  status {{ $labels.phase }}.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
              expr: >
                kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"}
                > 0
              for: 5m
              labels:
                severity: critical
        - name: kubernetes-system
          rules:
            - alert: KubeVersionMismatch
              annotations:
                message: >-
                  There are {{ $value }} different semantic versions of
                  Kubernetes components running.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
              expr: >
                count(count by (gitVersion)
                (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*")))
                > 1
              for: 15m
              labels:
                severity: warning
            - alert: KubeClientErrors
              annotations:
                message: >-
                  Kubernetes API server client '{{ $labels.job }}/{{
                  $labels.instance }}' is experiencing {{ $value |
                  humanizePercentage }} errors.'
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
              expr: >
                (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by
                (instance, job)
                  /
                sum(rate(rest_client_requests_total[5m])) by (instance, job))

                > 0.01
              for: 15m
              labels:
                severity: warning
        - name: kubernetes-system-apiserver
          rules:
            - alert: KubeAPILatencyHigh
              annotations:
                message: >-
                  The API server has a 99th percentile latency of {{ $value }}
                  seconds for {{ $labels.verb }} {{ $labels.resource }}.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
              expr: >
                cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}
                > 1
              for: 10m
              labels:
                severity: warning
            - alert: KubeAPILatencyHigh
              annotations:
                message: >-
                  The API server has a 99th percentile latency of {{ $value }}
                  seconds for {{ $labels.verb }} {{ $labels.resource }}.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
              expr: >
                cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}
                > 4
              for: 10m
              labels:
                severity: critical
            - alert: KubeAPIErrorsHigh
              annotations:
                message: >-
                  API server is returning errors for {{ $value |
                  humanizePercentage }} of requests.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
              expr: >
                sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
                  /
                sum(rate(apiserver_request_total{job="apiserver"}[5m])) > 0.03
              for: 10m
              labels:
                severity: critical
            - alert: KubeAPIErrorsHigh
              annotations:
                message: >-
                  API server is returning errors for {{ $value |
                  humanizePercentage }} of requests.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
              expr: >
                sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
                  /
                sum(rate(apiserver_request_total{job="apiserver"}[5m])) > 0.01
              for: 10m
              labels:
                severity: warning
            - alert: KubeAPIErrorsHigh
              annotations:
                message: >-
                  API server is returning errors for {{ $value |
                  humanizePercentage }} of requests for {{ $labels.verb }} {{
                  $labels.resource }} {{ $labels.subresource }}.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
              expr: >
                sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
                by (resource,subresource,verb)
                  /
                sum(rate(apiserver_request_total{job="apiserver"}[5m])) by
                (resource,subresource,verb) > 0.10
              for: 10m
              labels:
                severity: critical
            - alert: KubeAPIErrorsHigh
              annotations:
                message: >-
                  API server is returning errors for {{ $value |
                  humanizePercentage }} of requests for {{ $labels.verb }} {{
                  $labels.resource }} {{ $labels.subresource }}.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
              expr: >
                sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
                by (resource,subresource,verb)
                  /
                sum(rate(apiserver_request_total{job="apiserver"}[5m])) by
                (resource,subresource,verb) > 0.05
              for: 10m
              labels:
                severity: warning
            - alert: KubeClientCertificateExpiration
              annotations:
                message: >-
                  A client certificate used to authenticate to the apiserver is
                  expiring in less than 7.0 days.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
              expr: >
                apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
                > 0 and histogram_quantile(0.01, sum by (job, le)
                (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
                < 604800
              labels:
                severity: warning
            - alert: KubeClientCertificateExpiration
              annotations:
                message: >-
                  A client certificate used to authenticate to the apiserver is
                  expiring in less than 24.0 hours.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
              expr: >
                apiserver_client_certificate_expiration_seconds_count{job="apiserver"}
                > 0 and histogram_quantile(0.01, sum by (job, le)
                (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
                < 86400
              labels:
                severity: critical
            - alert: KubeAPIDown
              annotations:
                message: KubeAPI has disappeared from Prometheus target discovery.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown
              expr: |
                absent(up{job="apiserver"} == 1)
              for: 15m
              labels:
                severity: critical
        - name: kubernetes-system-kubelet
          rules:
            - alert: KubeNodeNotReady
              annotations:
                message: '{{ $labels.node }} has been unready for more than 15 minutes.'
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
              expr: >
                kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"}
                == 0
              for: 15m
              labels:
                severity: warning
            - alert: KubeNodeUnreachable
              annotations:
                message: >-
                  {{ $labels.node }} is unreachable and some workloads may be
                  rescheduled.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable
              expr: >
                kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"}
                == 1
              labels:
                severity: warning
            - alert: KubeletTooManyPods
              annotations:
                message: >-
                  Kubelet '{{ $labels.node }}' is running at {{ $value |
                  humanizePercentage }} of its Pod capacity.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
              expr: >
                max(max(kubelet_running_pod_count{job="kubelet"}) by(instance) *
                on(instance) group_left(node) kubelet_node_name{job="kubelet"})
                by(node) /
                max(kube_node_status_capacity_pods{job="kube-state-metrics"})
                by(node) > 0.95
              for: 15m
              labels:
                severity: warning
            - alert: KubeletDown
              annotations:
                message: Kubelet has disappeared from Prometheus target discovery.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown
              expr: |
                absent(up{job="kubelet"} == 1)
              for: 15m
              labels:
                severity: critical
        - name: kubernetes-system-scheduler
          rules:
            - alert: KubeSchedulerDown
              annotations:
                message: >-
                  KubeScheduler has disappeared from Prometheus target
                  discovery.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown
              expr: |
                absent(up{job="kube-scheduler"} == 1)
              for: 15m
              labels:
                severity: critical
        - name: kubernetes-system-controller-manager
          rules:
            - alert: KubeControllerManagerDown
              annotations:
                message: >-
                  KubeControllerManager has disappeared from Prometheus target
                  discovery.
                runbook_url: >-
                  https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown
              expr: |
                absent(up{job="kube-controller-manager"} == 1)
              for: 15m
              labels:
                severity: critical
        - name: prometheus
          rules:
            - alert: PrometheusBadConfig
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
                  reload its configuration.
                summary: Failed Prometheus configuration reload.
              expr: >
                # Without max_over_time, failed scrapes could create false
                negatives, see

                #
                https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0
                for details.

                max_over_time(prometheus_config_last_reload_successful{job="prometheus-k8s",namespace="monitoring"}[5m])
                == 0
              for: 10m
              labels:
                severity: critical
            - alert: PrometheusNotificationQueueRunningFull
              annotations:
                description: >-
                  Alert notification queue of Prometheus
                  {{$labels.namespace}}/{{$labels.pod}} is running full.
                summary: >-
                  Prometheus alert notification queue predicted to run full in
                  less than 30m.
              expr: >
                # Without min_over_time, failed scrapes could create false
                negatives, see

                #
                https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0
                for details.

                (
                  predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s",namespace="monitoring"}[5m], 60 * 30)
                >
                  min_over_time(prometheus_notifications_queue_capacity{job="prometheus-k8s",namespace="monitoring"}[5m])
                )
              for: 15m
              labels:
                severity: warning
            - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
              annotations:
                description: >-
                  {{ printf "%.1f" $value }}% errors while sending alerts from
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} to
                  Alertmanager {{$labels.alertmanager}}.
                summary: >-
                  Prometheus has encountered more than 1% errors sending alerts
                  to a specific Alertmanager.
              expr: |
                (
                  rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                /
                  rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                )
                * 100
                > 1
              for: 15m
              labels:
                severity: warning
            - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
              annotations:
                description: >-
                  {{ printf "%.1f" $value }}% minimum errors while sending
                  alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}}
                  to any Alertmanager.
                summary: >-
                  Prometheus encounters more than 3% errors sending alerts to
                  any Alertmanager.
              expr: |
                min without(alertmanager) (
                  rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                /
                  rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                )
                * 100
                > 3
              for: 15m
              labels:
                severity: critical
            - alert: PrometheusNotConnectedToAlertmanagers
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} is not
                  connected to any Alertmanagers.
                summary: Prometheus is not connected to any Alertmanagers.
              expr: >
                # Without max_over_time, failed scrapes could create false
                negatives, see

                #
                https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0
                for details.

                max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s",namespace="monitoring"}[5m])
                < 1
              for: 10m
              labels:
                severity: warning
            - alert: PrometheusTSDBReloadsFailing
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
                  {{$value | humanize}} reload failures over the last 3h.
                summary: Prometheus has issues reloading blocks from disk.
              expr: >
                increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s",namespace="monitoring"}[3h])
                > 0
              for: 4h
              labels:
                severity: warning
            - alert: PrometheusTSDBCompactionsFailing
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
                  {{$value | humanize}} compaction failures over the last 3h.
                summary: Prometheus has issues compacting blocks.
              expr: >
                increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s",namespace="monitoring"}[3h])
                > 0
              for: 4h
              labels:
                severity: warning
            - alert: PrometheusNotIngestingSamples
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} is not
                  ingesting samples.
                summary: Prometheus is not ingesting samples.
              expr: >
                rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                <= 0
              for: 10m
              labels:
                severity: warning
            - alert: PrometheusDuplicateTimestamps
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
                  {{ printf "%.4g" $value  }} samples/s with different values
                  but duplicated timestamp.
                summary: Prometheus is dropping samples with duplicate timestamps.
              expr: >
                rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                > 0
              for: 10m
              labels:
                severity: warning
            - alert: PrometheusOutOfOrderTimestamps
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
                  {{ printf "%.4g" $value  }} samples/s with timestamps arriving
                  out of order.
                summary: Prometheus drops samples with out-of-order timestamps.
              expr: >
                rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                > 0
              for: 10m
              labels:
                severity: warning
            - alert: PrometheusRemoteStorageFailures
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to
                  send {{ printf "%.1f" $value }}% of the samples to queue
                  {{$labels.queue}}.
                summary: Prometheus fails to send samples to remote storage.
              expr: |
                (
                  rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                /
                  (
                    rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                  +
                    rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                  )
                )
                * 100
                > 1
              for: 15m
              labels:
                severity: critical
            - alert: PrometheusRemoteWriteBehind
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
                  is {{ printf "%.1f" $value }}s behind for queue
                  {{$labels.queue}}.
                summary: Prometheus remote write is behind.
              expr: >
                # Without max_over_time, failed scrapes could create false
                negatives, see

                #
                https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0
                for details.

                (
                  max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="prometheus-k8s",namespace="monitoring"}[5m])
                - on(job, instance) group_right
                  max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="prometheus-k8s",namespace="monitoring"}[5m])
                )

                > 120
              for: 15m
              labels:
                severity: critical
            - alert: PrometheusRemoteWriteDesiredShards
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
                  desired shards calculation wants to run {{ $value }} shards,
                  which is more than the max of {{ printf
                  `prometheus_remote_storage_shards_max{instance="%s",job="prometheus-k8s",namespace="monitoring"}`
                  $labels.instance | query | first | value }}.
                summary: >-
                  Prometheus remote write desired shards calculation wants to
                  run more than configured max shards.
              expr: >
                # Without max_over_time, failed scrapes could create false
                negatives, see

                #
                https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0
                for details.

                (
                  max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-k8s",namespace="monitoring"}[5m])
                >
                  max_over_time(prometheus_remote_storage_shards_max{job="prometheus-k8s",namespace="monitoring"}[5m])
                )
              for: 15m
              labels:
                severity: warning
            - alert: PrometheusRuleFailures
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
                  evaluate {{ printf "%.0f" $value }} rules in the last 5m.
                summary: Prometheus is failing rule evaluations.
              expr: >
                increase(prometheus_rule_evaluation_failures_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                > 0
              for: 15m
              labels:
                severity: critical
            - alert: PrometheusMissingRuleEvaluations
              annotations:
                description: >-
                  Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{
                  printf "%.0f" $value }} rule group evaluations in the last 5m.
                summary: >-
                  Prometheus is missing rule evaluations due to slow rule group
                  evaluation.
              expr: >
                increase(prometheus_rule_group_iterations_missed_total{job="prometheus-k8s",namespace="monitoring"}[5m])
                > 0
              for: 15m
              labels:
                severity: warning
        - name: alertmanager.rules
          rules:
            - alert: AlertmanagerConfigInconsistent
              annotations:
                message: >-
                  The configuration of the instances of the Alertmanager cluster
                  `{{$labels.service}}` are out of sync.
              expr: >
                count_values("config_hash",
                alertmanager_config_hash{job="alertmanager-main",namespace="monitoring"})
                BY (service) / ON(service) GROUP_LEFT()
                label_replace(max(prometheus_operator_spec_replicas{job="prometheus-operator",namespace="monitoring",controller="alertmanager"})
                by (name, job, namespace, controller), "service",
                "alertmanager-$1", "name", "(.*)") != 1
              for: 5m
              labels:
                severity: critical
            - alert: AlertmanagerFailedReload
              annotations:
                message: >-
                  Reloading Alertmanager's configuration has failed for {{
                  $labels.namespace }}/{{ $labels.pod}}.
              expr: >
                alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="monitoring"}
                == 0
              for: 10m
              labels:
                severity: warning
            - alert: AlertmanagerMembersInconsistent
              annotations:
                message: Alertmanager has not found all other members of the cluster.
              expr: >
                alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"}
                  != on (service) GROUP_LEFT()
                count by (service)
                (alertmanager_cluster_members{job="alertmanager-main",namespace="monitoring"})
              for: 5m
              labels:
                severity: critical
        - name: general.rules
          rules:
            - alert: TargetDown
              annotations:
                message: >-
                  {{ printf "%.4g" $value }}% of the {{ $labels.job }} targets
                  in {{ $labels.namespace }} namespace are down.
              expr: >-
                100 * (count(up == 0) BY (job, namespace, service) / count(up)
                BY (job, namespace, service)) > 10
              for: 10m
              labels:
                severity: warning
            - alert: Watchdog
              annotations:
                message: >
                  This is an alert meant to ensure that the entire alerting
                  pipeline is functional.

                  This alert is always firing, therefore it should always be
                  firing in Alertmanager

                  and always fire against a receiver. There are integrations
                  with various notification

                  mechanisms that send a notification when this alert is not
                  firing. For example the

                  "DeadMansSnitch" integration in PagerDuty.
              expr: vector(1)
              labels:
                severity: none
        - name: node-time
          rules:
            - alert: ClockSkewDetected
              annotations:
                message: >-
                  Clock skew detected on node-exporter {{ $labels.namespace
                  }}/{{ $labels.pod }}. Ensure NTP is configured correctly on
                  this host.
              expr: |
                abs(node_timex_offset_seconds{job="node-exporter"}) > 0.05
              for: 2m
              labels:
                severity: warning
        - name: node-network
          rules:
            - alert: NodeNetworkInterfaceFlapping
              annotations:
                message: >-
                  Network interface "{{ $labels.device }}" changing it's up
                  status often on node-exporter {{ $labels.namespace }}/{{
                  $labels.pod }}"
              expr: >
                changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m])
                > 2
              for: 2m
              labels:
                severity: warning
        - name: prometheus-operator
          rules:
            - alert: PrometheusOperatorReconcileErrors
              annotations:
                message: >-
                  Errors while reconciling {{ $labels.controller }} in {{
                  $labels.namespace }} Namespace.
              expr: >
                rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator",namespace="monitoring"}[5m])
                > 0.1
              for: 10m
              labels:
                severity: warning
            - alert: PrometheusOperatorNodeLookupErrors
              annotations:
                message: >-
                  Errors while reconciling Prometheus in {{ $labels.namespace }}
                  Namespace.
              expr: >
                rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator",namespace="monitoring"}[5m])
                > 0.1
              for: 10m
              labels:
                severity: warning
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      labels:
        app: grafana
      name: '${APP_NAME_GRAFANA}-pvc'
      namespace: '${NAMESPACE}'
    spec:
      storageClassName: csi-cephfs-sc
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
  - apiVersion: v1
    kind: Service
    metadata:
      labels:
        app: grafana
      name: '${APP_NAME_GRAFANA}-service'
      namespace: '${NAMESPACE}'
    spec:
       ports:
       - name: http
         port: 3000
         targetPort: http
       selector:
         app: grafana
       type: NodePort
  - apiVersion: extensions/v1beta1
    kind: Ingress
    metadata:
      name: '${APP_NAME_GRAFANA}'
      namespace: '${NAMESPACE}'
    spec:
      ingressClassName: tmax
      rules:
        - host: grafana.${GATEWAY_ADDRESS}
          http:
            paths:
              - backend:
                  serviceName: '${APP_NAME_GRAFANA}-service'
                  servicePort: 9090
                path: /
                pathType: Prefix
    status:
      loadBalancer:
        ingress:
          - ip: '${INGRESS_IP}'
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: '${APP_NAME_GRAFANA}-account'
      namespace: '${NAMESPACE}'
  - apiVersion: monitoring.coreos.com/v1
    kind: ServiceMonitor
    metadata:
      name: '${APP_NAME_GRAFANA}'
      namespace: '${NAMESPACE}'
    spec:
      endpoints:
      - interval: 15s
        port: http
      selector:
        matchLabels:
          app: grafana
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      app: grafana
      labels: null
      name: '${APP_NAME_GRAFANA}'
      namespace: '${NAMESPACE}'
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: grafana
      template:
        metadata:
          creationTimestamp: null
          labels:
            app: grafana
        spec:
          nodeSelector:
            beta.kubernetes.io/os: linux
          restartPolicy: Always
          serviceAccountName: '${APP_NAME_GRAFANA}-account'
          schedulerName: default-scheduler
          terminationGracePeriodSeconds: 30
          securityContext:
            runAsUser: 65534
            runAsNonRoot: true
          containers:
            - resources:
                limits:
                  cpu: 200m
                  memory: 200Mi
                requests:
                  cpu: 100m
                  memory: 100Mi
              readinessProbe:
                httpGet:
                  path: /api/health
                  port: http
                  scheme: HTTP
                timeoutSeconds: 1
                periodSeconds: 10
                successThreshold: 1
                failureThreshold: 3
              terminationMessagePath: /dev/termination-log
              name: grafana
              ports:
                - name: http
                  containerPort: 3000
                  protocol: TCP
              imagePullPolicy: IfNotPresent
              volumeMounts:
                - name: '${APP_NAME_GRAFANA}-pvc'
                  mountPath: /var/lib/grafana
              terminationMessagePolicy: File
              image: 'grafana/grafana:6.4.3'
          serviceAccount: grafana
          volumes:
            - name: '${APP_NAME_GRAFANA}-pvc'
              persistentVolumeClaim:
                claimName: '${APP_NAME_GRAFANA}-pvc'
          dnsPolicy: ClusterFirst
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 25%
          maxSurge: 25%
      revisionHistoryLimit: 10
      progressDeadlineSeconds: 600
status:
  observedGeneration: 6
  replicas: 1
  updatedReplicas: 1
  readyReplicas: 1
  availableReplicas: 1
  conditions:
    - type: Progressing
      status: 'True'
      lastUpdateTime: '2020-07-06T03:15:38Z'
      lastTransitionTime: '2020-05-29T03:16:50Z'
      reason: NewReplicaSetAvailable
      message: ReplicaSet "grafana-5445b8fd6f" has successfully progressed.
    - type: Available
      status: 'True'
      lastUpdateTime: '2020-07-06T03:17:13Z'
      lastTransitionTime: '2020-07-06T03:17:13Z'
      reason: MinimumReplicasAvailable
      message: Deployment has minimum availability.
parameters:
  - description: A Prometheus Name
    displayName: prometheus
    name: APP_NAME
    required: true
  - description: A Grafana Name
    displayName: grafana
    name: APP_NAME_GRAFANA
    required: true
  - description: Storage size
    displayName: gateway address
    name: GATEWAY_ADDRESS
    required: true
  - description: Storage size
    displayName: ingress name
    name: INGRESS_IP
    required: true
  - description: Namespace
    displayName: monitoring2
    name: NAMESPACE
    required: true
plans:
  - metadata:
      bullets:
        - Prometheus Deployment Plan
      costs:
        amount: 100
        unit: $
    bindable: true
    description: Prometheus Grafana Plan
    free: false
    name: prometheus-plan1
    plan_updateable: false
    schemas:
      service_instance:
        create:
          parameters:
            APP_NAME: prometheus-deploy
            STORAGE: 10Gi
provider: tmax
shortDescription: Prometheus Deployment
tags:
  - prometheus
  - grafana
